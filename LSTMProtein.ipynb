{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "LSTMProtein.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yY1k0LLPW-e",
        "colab_type": "text"
      },
      "source": [
        "This notebook contains our LSTM model for the protein data set and relevant pre processing functions to be able to use our data sets with the model, furthermore the test loss and perplexity can be computed in the bottom of the notebook with the possibility of generating new sequences through sampling.\n",
        "\n",
        "Authors: Mathias Dizon Olsson s174031, Mathias Sabroe Simonsen s164034"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-aBDL7Z7N28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Combine the domains into single test, training and validation splits\n",
        "domain_dir = [['/domain_set/virus/test.txt',\n",
        "               '/domain_set/virus/train.txt',\n",
        "               '/domain_set/virus/valid.txt'],\n",
        "              ['/domain_set/archaea/test.txt',\n",
        "               '/domain_set/archaea/train.txt',\n",
        "               '/domain_set/archaea/valid.txt'],\n",
        "              ['/domain_set/bacteria/test.txt',\n",
        "               '/domain_set/bacteria/train.txt',\n",
        "               '/domain_set/bacteria/valid.txt'],\n",
        "              ['/domain_set/eukaryotes/test.txt',\n",
        "               '/domain_set/eukaryotes/train.txt',\n",
        "               '/domain_set/eukaryotes/valid.txt'],\n",
        "              ['/domain_set/completenoeuk/test.txt',\n",
        "               '/domain_set/completenoeuk/train.txt',\n",
        "               '/domain_set/completenoeuk/valid.txt']]\n",
        "\n",
        "domains = ['virus',\n",
        "           'archaea',\n",
        "           'bacteria',\n",
        "           'eukaryotes',\n",
        "           'completenoeuk']\n",
        "\n",
        "# Combining everything but the eukaryotes as described in the paper\n",
        "read_files = [[domain_dir[0][0],domain_dir[1][0],domain_dir[2][0]],\n",
        "              [domain_dir[0][1],domain_dir[1][1],domain_dir[2][1]],\n",
        "              [domain_dir[0][2],domain_dir[1][2],domain_dir[2][2]]]\n",
        "\n",
        "for i in range(len(read_files)):\n",
        "    with open(domain_dir[4][i], \"wb\") as outfile:\n",
        "        for f in read_files[i]:\n",
        "            with open(f, \"rb\") as infile:\n",
        "                outfile.write(infile.read())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLVHGEU4GAt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ignore the specific warning in regards to module weights as explained in the discussions chapter of the paper\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Adfwv1BUqC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "import math\n",
        "\n",
        "import torch\n",
        "\n",
        "import time\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "dtype = torch.FloatTensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LunCmMKwDQmJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.nn import Parameter\n",
        "from functools import wraps\n",
        "\n",
        "# Modified WeightDrop function based on the function made by salesforce (https://github.com/salesforce/) found here -> https://github.com/ChengyueGongR/awd-lstm-lm/blob/master/weight_drop.py\n",
        "# using the modifications made in the post -> https://github.com/salesforce/awd-lstm-lm/issues/79 by github user -> https://github.com/shirishr\n",
        "class WeightDrop(torch.nn.Module):\n",
        "    def __init__(self, module, weights, dropout=0, variational=False):\n",
        "        super(WeightDrop, self).__init__()\n",
        "        self.module = module\n",
        "        self.weights = weights\n",
        "        self.dropout = dropout\n",
        "        self.variational = variational\n",
        "        self._setup()\n",
        "\n",
        "    def widget_demagnetizer_y2k_edition(*args, **kwargs):\n",
        "        # We need to replace flatten_parameters with a nothing function\n",
        "        # It must be a function rather than a lambda as otherwise pickling explodes\n",
        "        # We can't write boring code though, so ... WIDGET DEMAGNETIZER Y2K EDITION!\n",
        "        # (╯°□°）╯︵ ┻━┻\n",
        "        return\n",
        "\n",
        "    def _setup(self):\n",
        "        # Terrible temporary solution to an issue regarding compacting weights re: CUDNN RNN\n",
        "        if issubclass(type(self.module), torch.nn.RNNBase):\n",
        "            self.module.flatten_parameters = self.widget_demagnetizer_y2k_edition\n",
        "\n",
        "        for name_w in self.weights:\n",
        "            print('Applying weight drop of {} to {}'.format(self.dropout, name_w))\n",
        "            w = getattr(self.module, name_w)\n",
        "            del self.module._parameters[name_w]\n",
        "            self.module.register_parameter(name_w + '_raw', Parameter(w.data))\n",
        "\n",
        "    def _setweights(self):\n",
        "        for name_w in self.weights:\n",
        "            raw_w = getattr(self.module, name_w + '_raw')\n",
        "            w = None\n",
        "            if self.variational:\n",
        "                mask = torch.autograd.Variable(torch.ones(raw_w.size(0), 1))\n",
        "                if raw_w.is_cuda: mask = mask.cuda()\n",
        "                mask = torch.nn.functional.dropout(mask, p=self.dropout, training=True)\n",
        "            # the modified code    \n",
        "                w = torch.nn.Parameter(mask.expand_as(raw_w) * raw_w)\n",
        "            else:\n",
        "                w = torch.nn.Parameter(torch.nn.functional.dropout(raw_w, p=self.dropout, training=self.training))\n",
        "            # the original code\n",
        "            #    w = mask.expand_as(raw_w) * raw_w\n",
        "            #else:\n",
        "            #    w = torch.nn.functional.dropout(raw_w, p=self.dropout, training=self.training)\n",
        "            setattr(self.module, name_w, w)\n",
        "\n",
        "    def forward(self, *args):\n",
        "        self._setweights()\n",
        "        return self.module.forward(*args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pikmDcLIUtl0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to select which dataset should be used\n",
        "def domain_select(domain, field, rootpath):\n",
        "    \n",
        "    splits = ['train',\n",
        "              'valid',\n",
        "              'test']\n",
        "    split = [None]*3\n",
        "\n",
        "    for i in range(3):\n",
        "        path = rootpath + '/' + domain + '/' + splits[i] + '.txt'\n",
        "        split[i] = torchtext.datasets.LanguageModelingDataset(path = path, \n",
        "                                                              text_field = field)\n",
        "\n",
        "    return (split[0],split[1],split[2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKndNeayQIaq",
        "colab_type": "text"
      },
      "source": [
        "The desired domain can be chosen by changing the domain field to the available domains in the code comment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qT4Eiq_uDY3d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set up field, path and domain\n",
        "TEXT = data.Field(lower=True, batch_first=False)\n",
        "path = '/content/drive/My Drive/Deep Learning Project/domain_set'\n",
        "domain = 'completenoeuk' # Available domains: virus, archaea, bacteria, eukaryotes (might crash due to size), completenoeuk (a combined set of all domains but eukaryotes)\n",
        "\n",
        "# create torchtext datasets\n",
        "train, valid, test = domain_select(domain = domain,\n",
        "                                   field = TEXT,\n",
        "                                   rootpath = path)\n",
        "\n",
        "# build the vocabulary\n",
        "TEXT.build_vocab(train)\n",
        "\n",
        "# batch size and backpropagate-through-time length\n",
        "batch_size = 20\n",
        "bptt_len = 600\n",
        "\n",
        "# create iterator for splits\n",
        "train_iter, valid_iter, test_iter = data.BPTTIterator.splits((train, valid, test), \n",
        "                                                              batch_sizes = (batch_size, batch_size, 1), \n",
        "                                                              bptt_len = bptt_len, \n",
        "                                                              device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zY8Tf5glDY3p",
        "colab_type": "code",
        "outputId": "f8a4673a-649e-4477-a675-daae1e7d2457",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Display the vocabulary\n",
        "for i in range(len(TEXT.vocab)):\n",
        "    if i < len(TEXT.vocab)-1:\n",
        "        print(TEXT.vocab.itos[i], end = ', ')\n",
        "    else:\n",
        "        print(TEXT.vocab.itos[i], end = '.') "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<unk>, <pad>, l, a, e, g, v, k, d, i, t, s, r, p, n, q, f, y, m, h, w, c, <eos>, x, b, u, z, o."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ej-QxWn2DY31",
        "colab_type": "code",
        "outputId": "cc6b92aa-1635-4bad-83df-66e20d5c54df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Check amount of batches\n",
        "validation_set = (iter(valid_iter))\n",
        "training_set = (iter(train_iter))\n",
        "test_set = (iter(test_iter))\n",
        "\n",
        "validation_batch_amount = 0\n",
        "while True:\n",
        "    try:\n",
        "        c = (next(validation_set).text)\n",
        "    except StopIteration:\n",
        "        print('Amount of validation text batches: ',validation_batch_amount)\n",
        "        break  # Iterator exhausted: stop the loop\n",
        "    else:\n",
        "        validation_batch_amount = validation_batch_amount + 1\n",
        "\n",
        "training_batch_amount = 0\n",
        "while True:\n",
        "    try:\n",
        "        c = (next(training_set).text)\n",
        "    except StopIteration:\n",
        "        print('Amount of training batches: ',training_batch_amount)\n",
        "        break  # Iterator exhausted: stop the loop\n",
        "    else:\n",
        "        training_batch_amount = training_batch_amount + 1\n",
        "        \n",
        "test_batch_amount = 0\n",
        "while True:\n",
        "    try:\n",
        "        c = (next(test_set).text)\n",
        "    except StopIteration:\n",
        "        print('Amount of test batches: ',test_batch_amount)\n",
        "        break  # Iterator exhausted: stop the loop\n",
        "    else:\n",
        "        test_batch_amount = test_batch_amount + 1      "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Amount of validation text batches:  358\n",
            "Amount of training batches:  2579\n",
            "Amount of test batches:  19098\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip2cAOjRDY37",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reset iterators\n",
        "validation_set = (iter(valid_iter))\n",
        "training_set = (iter(train_iter))\n",
        "test_set = (iter(test_iter))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SX2myxwRtyMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, weights, dropout):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.embed = nn.Embedding(num_embeddings = vocab_size, \n",
        "                                  embedding_dim = embed_size)\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size = embed_size, \n",
        "                            hidden_size = hidden_size, \n",
        "                            num_layers = num_layers, \n",
        "                            batch_first = False)\n",
        "\n",
        "        self.weightdrop = WeightDrop(module = self.lstm, \n",
        "                                     weights = weights, \n",
        "                                     dropout = dropout)\n",
        "        \n",
        "        self.l_out = nn.Linear(in_features = hidden_size, \n",
        "                               out_features = vocab_size, \n",
        "                               bias = False)\n",
        "        \n",
        "    def forward(self, x, hc):\n",
        "        # Embed word ids to vectors\n",
        "        x = self.embed(x)\n",
        "        \n",
        "        # Weightdropped LSTM returns output and last hidden state\n",
        "        out, (h, c) = self.weightdrop(x, hc)\n",
        "\n",
        "        # Reshape output to (sequence_length*batch_size, hidden_size)\n",
        "        out = out.reshape(out.shape[0]*out.shape[1], -1)\n",
        "\n",
        "        # Decode hidden states of all time steps\n",
        "        out = self.l_out(out)\n",
        "\n",
        "        return out, (h, c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apf_TRc3tz_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyper-parameters\n",
        "num_epochs = 10\n",
        "embed_size = 25\n",
        "hidden_size = 128\n",
        "num_layers = 3\n",
        "learning_rate = 0.002\n",
        "max_norm = 0.25\n",
        "weights = ['weight_hh_l' + str(i) for i in range(num_layers)]\n",
        "dropout = 0.9\n",
        "batch_size = 20\n",
        "switch = False\n",
        "vocab_size = len(TEXT.vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWXH9i4Ruh0A",
        "colab_type": "code",
        "outputId": "5caf1e9a-6848-4a6a-bccf-e4d9a3c644f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "# Initialize a new network\n",
        "net = Net(vocab_size, embed_size, hidden_size, num_layers, weights, dropout).to(device)\n",
        "print(net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Applying weight drop of 0.9 to weight_hh_l0\n",
            "Applying weight drop of 0.9 to weight_hh_l1\n",
            "Applying weight drop of 0.9 to weight_hh_l2\n",
            "Net(\n",
            "  (embed): Embedding(28, 25)\n",
            "  (lstm): LSTM(25, 128, num_layers=3)\n",
            "  (weightdrop): WeightDrop(\n",
            "    (module): LSTM(25, 128, num_layers=3)\n",
            "  )\n",
            "  (l_out): Linear(in_features=128, out_features=28, bias=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BShWQo-DY4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(params=net.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTqyJH_N2gcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Detach hidden and cell states of the LSTM\n",
        "def detach(states):\n",
        "    return [state.detach() for state in states]\n",
        "\n",
        "# Weight initialization\n",
        "def weight_init():\n",
        "    return (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n",
        "            torch.zeros(num_layers, batch_size, hidden_size).to(device))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kljzt9FTDY4O",
        "colab_type": "code",
        "outputId": "6d7d3f66-be71-483c-84e3-792401f4ffe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Training loop\n",
        "# Track loss\n",
        "training_loss, validation_loss = [], []\n",
        "\n",
        "NT_ASGD_loss = []\n",
        "\n",
        "# For each epoch\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    # Display epoch number\n",
        "    if epoch == 0:\n",
        "        print(f'Epoch {epoch+1} of {num_epochs}')\n",
        "    else:\n",
        "        print(f'\\nEpoch {epoch+1} of {num_epochs}')\n",
        "        \n",
        "    # Track loss\n",
        "    epoch_training_loss = 0\n",
        "    epoch_validation_loss = 0\n",
        "    \n",
        "    # Validation\n",
        "    net.eval()\n",
        "    \n",
        "    hc = weight_init()\n",
        "\n",
        "    batch_num = 0\n",
        "    print('Net validation')\n",
        "\n",
        "    for batch in valid_iter:\n",
        "        \n",
        "        batch_num = batch_num + 1  \n",
        "        \n",
        "        text = (batch.text).to(device)\n",
        "        target = (batch.target).to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        hc = detach(hc)\n",
        "        outputs, hc = net(text, hc)\n",
        "        loss = criterion(outputs, target.reshape(-1))\n",
        "        \n",
        "        \n",
        "        # Update loss\n",
        "        epoch_validation_loss += loss.detach()\n",
        "\n",
        "        # Loading bar\n",
        "        sys.stdout.write('\\r')\n",
        "        p = (batch_num) / validation_batch_amount\n",
        "        sys.stdout.write(\"[%-50s] %d%%\" % ('='*int(50*p), 100*p))\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    # Training\n",
        "    net.train()    \n",
        "\n",
        "    hc = weight_init()\n",
        "\n",
        "    batch_num = 0\n",
        "    print('\\nNet training')\n",
        "                  \n",
        "    for batch in train_iter:\n",
        "        batch_num = batch_num + 1\n",
        "\n",
        "        text = (batch.text).to(device)\n",
        "        target = (batch.target).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        hc = detach(hc)\n",
        "        outputs, hc = net(text, hc)\n",
        "        loss = criterion(outputs, target.reshape(-1))\n",
        "        \n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        clip_grad_norm_(net.parameters(), max_norm)\n",
        "        optimizer.step()\n",
        "      \n",
        "        # Update loss\n",
        "        epoch_training_loss += loss.detach()\n",
        "        \n",
        "        # Loading bar\n",
        "        sys.stdout.write('\\r')\n",
        "        p = (batch_num) / training_batch_amount\n",
        "        sys.stdout.write(\"[%-50s] %d%%\" % ('='*int(50*p), 100*p))\n",
        "        sys.stdout.flush()\n",
        "        \n",
        "\n",
        "    # Save loss for plot\n",
        "    training_loss.append(epoch_training_loss/(training_batch_amount))\n",
        "    validation_loss.append(epoch_validation_loss/(validation_batch_amount))\n",
        "\n",
        "    # Print loss every epoch\n",
        "    print(f'\\nTraining loss: {training_loss[-1]}, Validation loss: {validation_loss[-1]}')\n",
        "\n",
        "    # NT-ASGD\n",
        "    NT_ASGD_loss.append(validation_loss[-1].cpu())\n",
        "    if len(NT_ASGD_loss) == 6:\n",
        "        del NT_ASGD_loss[0]\n",
        "        if switch == False:\n",
        "            if np.isclose(NT_ASGD_loss, NT_ASGD_loss[4], rtol=0, atol = 5e-4).all() == True:\n",
        "                print('\\nOptimizer has changed to ASGD')\n",
        "                switch = True\n",
        "                optimizer = optim.ASGD(params=net.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 of 10\n",
            "Net validation\n",
            "[==================================================] 100%\n",
            "Net training\n",
            "[==================================================] 100%\n",
            "Training loss: 3.313389778137207, Validation loss: 3.334681272506714\n",
            "\n",
            "Epoch 2 of 10\n",
            "Net validation\n",
            "[==================================================] 100%\n",
            "Net training\n",
            "[==================================================] 100%\n",
            "Training loss: 3.2634942531585693, Validation loss: 3.2883694171905518\n",
            "\n",
            "Epoch 3 of 10\n",
            "Net validation\n",
            "[==================================================] 100%\n",
            "Net training\n",
            "[==================================================] 100%\n",
            "Training loss: 3.1988396644592285, Validation loss: 3.2248568534851074\n",
            "\n",
            "Epoch 4 of 10\n",
            "Net validation\n",
            "[==================================================] 100%\n",
            "Net training\n",
            "[==================================================] 100%\n",
            "Training loss: 3.128234386444092, Validation loss: 3.1467931270599365\n",
            "\n",
            "Epoch 5 of 10\n",
            "Net validation\n",
            "[==================================================] 100%\n",
            "Net training\n",
            "[==================================================] 100%\n",
            "Training loss: 3.060789108276367, Validation loss: 3.0677640438079834\n",
            "\n",
            "Epoch 6 of 10\n",
            "Net validation\n",
            "[==================================================] 100%\n",
            "Net training\n",
            "[==================================================] 100%\n",
            "Training loss: 3.0058393478393555, Validation loss: 3.001575469970703\n",
            "\n",
            "Epoch 7 of 10\n",
            "Net validation\n",
            "[==================================================] 100%\n",
            "Net training\n",
            "[==================================================] 100%\n",
            "Training loss: 2.967348098754883, Validation loss: 2.9563148021698\n",
            "\n",
            "Epoch 8 of 10\n",
            "Net validation\n",
            "[==================================================] 100%\n",
            "Net training\n",
            "[==================================================] 100%\n",
            "Training loss: 2.9443562030792236, Validation loss: 2.93093204498291\n",
            "\n",
            "Epoch 9 of 10\n",
            "Net validation\n",
            "[==================================================] 100%\n",
            "Net training\n",
            "[==================================================] 100%\n",
            "Training loss: 2.9296019077301025, Validation loss: 2.9169018268585205\n",
            "\n",
            "Epoch 10 of 10\n",
            "Net validation\n",
            "[==================================================] 100%\n",
            "Net training\n",
            "[==================================================] 100%\n",
            "Training loss: 2.9202358722686768, Validation loss: 2.9080750942230225\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coUvqt1Q7LT8",
        "colab_type": "code",
        "outputId": "37052493-0877-4439-bed1-7c023a0f1c70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Compute the test loss and perplexity\n",
        "complete_test_loss = []\n",
        "test_loss = 0\n",
        "batch_num = 0\n",
        "\n",
        "for batch in test_iter:\n",
        "\n",
        "    hc = (torch.zeros(num_layers, 1, hidden_size).to(device),\n",
        "          torch.zeros(num_layers, 1, hidden_size).to(device))\n",
        "    \n",
        "    batch_num = batch_num + 1\n",
        "    text = (batch.text).to(device)\n",
        "    target = (batch.target).to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    hc = detach(hc)\n",
        "    outputs, hc = net(text, hc)\n",
        "    loss = criterion(outputs, target.reshape(-1))\n",
        "\n",
        "    # Update loss\n",
        "    test_loss += loss.detach()\n",
        "\n",
        "    # Loading bar\n",
        "    sys.stdout.write('\\r')\n",
        "    p = (batch_num) / test_batch_amount\n",
        "    sys.stdout.write(\"[%-50s] %d%%\" % ('='*int(50*p), 100*p))\n",
        "    sys.stdout.flush()\n",
        "\n",
        "complete_test_loss = test_loss / test_batch_amount\n",
        "print(f'\\ntest loss: {complete_test_loss}, perplexity: {np.exp(complete_test_loss.cpu())}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100%\n",
            "test_loss: 2.9294137954711914, perplexity: 18.716655731201172\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBXGs1wsbzPH",
        "colab_type": "code",
        "outputId": "7c7b3c6d-3d73-40bf-c222-7780703c561f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Sampling to a txt file\n",
        "\n",
        "num_samples = 2000\n",
        "\n",
        "with torch.no_grad():\n",
        "    with open('completenoeuk_results.txt', 'w') as f:\n",
        "        \n",
        "        # Set intial hidden and cell states\n",
        "        hc = (torch.zeros(num_layers, 1, hidden_size).to(device),\n",
        "                 torch.zeros(num_layers, 1, hidden_size).to(device))\n",
        "\n",
        "        # Starting word, in this case the amino acid M\n",
        "        input = torch.Tensor([[18]]).to(device).long()\n",
        "\n",
        "        for i in range(num_samples):\n",
        "\n",
        "            # Forward pass\n",
        "            output, hc = net(input, hc)\n",
        "\n",
        "            # Sample word\n",
        "            prob = output.exp()\n",
        "            word_id = torch.multinomial(prob, num_samples=1).item()\n",
        "\n",
        "            # Fill input with sampled word for the next time step\n",
        "            input.fill_(word_id)\n",
        "\n",
        "            # File write\n",
        "            if i == 0:\n",
        "                f.write(f'perplexity: {np.exp(complete_test_loss.cpu())}' + '\\n')\n",
        "                f.write('Sequence: ' + TEXT.vocab.itos[18] + ' ') \n",
        "            word = TEXT.vocab.itos[word_id]\n",
        "            word = '\\n' + 'Sequence: ' if word == '<eos>' else word + ' '\n",
        "            f.write(word)\n",
        "\n",
        "    print('Words have been sampled and saved to .txt file')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words have been sampled and saved to .txt file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrIaCW3WErdG",
        "colab_type": "code",
        "outputId": "d40fa2e6-135c-4100-ad1b-1d20d80a19d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "# Sampling to python output\n",
        "\n",
        "num_samples = 2000\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Set intial hidden and cell states\n",
        "    hc = (torch.zeros(num_layers, 1, hidden_size).to(device),\n",
        "              torch.zeros(num_layers, 1, hidden_size).to(device))\n",
        "\n",
        "    # Starting word, in this case the amino acid M\n",
        "    input = torch.Tensor([[18]]).to(device).long()\n",
        "\n",
        "    for i in range(num_samples):\n",
        "\n",
        "        # Forward pass\n",
        "        output, hc = net(input, hc)\n",
        "\n",
        "        # Sample word\n",
        "        prob = output.exp()\n",
        "        word_id = torch.multinomial(prob, num_samples=1).item()\n",
        "\n",
        "        # Fill input with sampled word for the next time step\n",
        "        input.fill_(word_id)\n",
        "\n",
        "        # File write\n",
        "        if i == 0:\n",
        "            print('Sequence: ' + TEXT.vocab.itos[18], end = ' ') \n",
        "        word = TEXT.vocab.itos[word_id]\n",
        "        word = '\\n' + 'Sequence: ' if word == '<eos>' else word\n",
        "        print(word, end = ' ')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequence: w a g q q g r i t d s l a i l q r l v m v r i e n y n e n k v a k l i k r t l v l d d d l g v f d s e i v t t e a p l a v f d a d v l i l n m s i w s h d l a v e s n d s a r h q f v i a r n h g t l s r g l r d l i v p a i p i e e l a k r i m v g t v g a k a a g r k l t d k m r d k a n g i i f e v l d t a a s r g v l a i i v a a v q l y a a a v w l i t v e k d a t l t c a h i r e e n e f d p a l l a y g g v n r e h a n c l l <pad> v a v i i p l g k t p r q n i n g e k k v m f v q d a f k t a a a p g n k k n s v a a a e t m g r e i a s l a y r y k a q g k a v y f n a d a i h e k l w d e k g k n g i a p i g r k k d s q y p g v a v a r f g l k p a s k r h e g i k q f l i n a v e l a p g l g l k e t a p h e l q r r t v k v q e k m e l s s t f t a g k p v l v k v k m g r g l e r q n m q v q l f m i w k v m a p a v v \n",
            "Sequence:  g v t v f g e h v m e d a g a n a d g k e e p e t a w v e h n y g i d a r s i s v r h a g a q k e k a e g n d a l f k e l v t c g a k v p <pad> s k s d l p v g d q l e a g v y q s t p h g s q a v l v h s d v l s i s v n y g l e k l y k l r e n p e d p d a r l q p i r g e g p l t f t k v a y e a g k t k a d e s v e d s e i w w v l l i v q t s g r k a l l k s g h p k g t e l v t f e a g d k q a e p d k d i k l g i a h a r n d i i l q l e h a a s a v t v a e p d e p s q g k t s d e r g t v e g a i e k y g v a d s a l v a q a d a s q l q r a p t l t v a g n v k k l g t v s v y e g s c h q a v e g q g i l a g q t e p e k a g q p n i g r a l v v k r p m k d n v g a a a m s v e g d e m r a e s a v k l k g k s e k w l q h t g i z v a v k g d k p i k t s d e s q k k g s v i a l k v v r q r v n k r q n a h f s d a p f g h k t d p g l v n g w e h d e a g g s v p a a l a t d m z g i v e e g v v r v k t e q r v s a i k h t v i v e l g s d h v s i k e e p i s f q n t i t a t r i a g i a d d g l i n l k r g a f k p d f m g k k l s d d f a p n r q e d d e g r i s y e i g e e t m h f a i f t g k d t p t d d g i d g y e l l s f i a k f k l d d h i s a g m l e a m i s g k l k i g a g a p h i g q a l k l v d a l s r p e q e e v r f a k a m r n v l k a g d g r g e d k v q a t h l k a v n n l l l e a g l r s a l i g v d a y p s i q l k v r a f r q d t i v q k t t e r i n p r a f a g q k i a q e e e d e i s l e d a k a r l q g v a v a d e e k p t i v g n k a c d e m s g v k a i k g g e s l l n k d i y d l e s a l s e s t t a l e p r a l p i r a y e a n n g t a d d s v e d p v t y d m g a e h y v k a s s g i n u l a s l g a p s t l v e a l p a v v k a k y a e t t k h l t r d a f d s p a e l p v p g v d i p t d g v k l l d e k k l d p l d i y i m l n s a k t g v r l d v k i e v r l t t i q g f d e t l v l i i r k k i q e n s a y a i g i a l e s s a y l y y k l l q k q f t f i v l l g h q l f l a g q n s p y v m a y k w i a n k e n q n g l k n l e v r l l k r q q p l m e v r a t e t a l l t d k e n t e n m l i d v a a q e e d v m e l q l t a k p p l t v p g e r i i l a v g f a e s v q l s i f r v l t v f r g h n s q a r k a t e a t r d r e r r f t y a f v l v f m k p m r t y e l q a g f g y y q g d e q r y g t e q e q l p p k p n e r r k g h r a y v l m d k t d l g h p i i n e y s e g v h i a e a i e n t s v f t l t t g l f r l v d i e l s t p e g h a t y r y e a n t v i v i e g k t a t a p l n w l k l e g l r n l l v k a s a y v i a v r k k a v r p p r k s s v q r f n l v s l q i m w s k i e g r v l g p g l v a e n g e n l i y d l e v i e e m v g a l l i l k i q d p y y l f a g s t v g h d t s l d g k k w k m l v e n g i <pad> t g r l s n e d d l l n v d t l r l k e a p a q a i q i n i v g l a g i h e h g l e g s e t a l l r g v l t r y m i i r l p d f a g y l t u a a l e g q l g k e t i k k f s r m y k e m v m d e y e e r k d a s s d v e g t i l v e i t r q k v v h r a r f e l q t l t s t d p k l t a g l r q q l a y t g l f g g p i v d a l r a v t r s g l v a p i l q d k i a g v a t n d a m l p r g d a l t a i a i m c a d l a a s s s l t p g a e p r k a <unk> i q e l k g d e l k t a a p t i r p e d i a e a e d p g a l p q k k s t i s d a n f t r q a i g d l d "
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}